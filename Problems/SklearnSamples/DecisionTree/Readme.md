## Деревья решений

Строим бинарное дерево, где в каждой вершине предикат вида

$$
I[x_i^j < b]
$$

Нам остается только определить параметры и мы получим дерево.
Для этого, имея родительское дерево, будем делить его на два поддерева как раз
этим предикатом. Получим предикат, минимизируя такую функцию:

$$
IG(D_p, f) = I(D_p) - \frac{N_{left}}{N_{parent}}I(D_{left}) - \frac{N_{right}}{N_{parent}}I(D_right)
$$

Здесь мы используем:

* ``I`` - мера неодородности
* ``D`` - множество признаков родителя или нового поддерева.
* ``N`` - кол признаков у кого-нибудь

Остается только ввести меру неоднородности:

* Мера Джини

$$
I_G(t) = 1 - \sum_{i=1}^c p(i|t)^2
$$

Здесь p(i|t) - доля образцов, принадлежащих классу i отдельно взятого узла t.

* Энтропия

$$
I_H(t) = -\sum_i^c p(i|t)\log_2p(i|t)
$$

На практике эти два варианта дают очень похожие результаты

## Параметры

У нас есть класс DecisionTreeClassifier.

* ``criterion`` - выбираем критерий для деления - энтропия или мера Джини
* ``max_depth`` - максимальная глубина дерева