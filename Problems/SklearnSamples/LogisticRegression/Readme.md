## Логистическая регрессия

Исользуем такую же весовую линейную модель, как и в персептроне.
Но теперь функция активации будет сигмойдой.

$$
\sigma((\vec w \cdot \vec x)) = \frac{1}{1+\exp{-(\vec w \cdot \vec x)}}
$$

Причем, выход этой сигмойды можно трактовать, как вероятность пренадлежности соответствующему классу.
В остальном в основе этого метода лежит такой же рекурсивный спуск, как и в персептроне.   

Но в данном случае, наше мы будем минимизировать логарифмическое правдоподобие.

$$
J(\vec w) = -\ln L(\vec w) = -\ln P(\vec y | \vec x \vec w) = 
-\ln \prod_{i=1}^{n} (\sigma((\vec w \cdot \vec x_i)))^{y^{(i)}}(1-\sigma((\vec w \cdot \vec x_i)))^{1-y^{(i)}}
$$

Отметим, что последний переход есть определение правдоподобия для двух классов. То есть сигмойда
- это вероятность найди класс 1. Если yi равно 1, значит нам и нужна вероятность найти этот класс,
а если yi равно 0, то мы смотрим на вероятность найти класс 0, то есть вычитаем из единицы.      

### Регуляризация

Мы хотим накладывать штрафы за экстремальные веса параметров.
Для этого к функции стоимости, которую минимизируем прибавим член, линейный к какой-нибудь норме.

J(\vec w) = -\ln L(\vec w) = -\ln P(\vec y | \vec x \vec w) = 
-\ln \prod_{i=1}^{n} (\sigma((\vec w \cdot \vec x_i)))^{y^{(i)}}(1-\sigma((\vec w \cdot \vec x_i)))^{1-y^{(i)}}
+ \frac{\lambda}{2}||\vec w||_{L_i}

### Полезные параметры

В билиотеке есть класс ``LogisticRegression``. Рассмотрим его параметры.

* ``penalty`` - вид регуляризации
* ``C`` - параметр, обратный к лямбда. Чем он меньше, тем сильнее регуляризация.