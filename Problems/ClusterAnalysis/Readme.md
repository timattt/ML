# Кластерный анализ

Обучение без учителя. Здесь мы хотим разбить выборку
на кластеры.

## Алгоритм k-средних

1. Случайно выбрать k точек - которые будем считать центроидами.
2. Каждый объект сопоставить ближайшему к нему центройду.
3. Переместить каждый центроид в центр образцов, которые были ему сопоставлены.
4. Повторять предыдущее, пока назначения кластеров не перестанут меняться.

Реализация в классе ```KMeans```,
параметр ```init``` нужно ставить в random.

## Алгоритм k-средних++

1. Завести пустое множество центроидов M.
2. Случайно выбрать из входных данных первый центроид и поместить в M.
3. Для каждого образца, который не лежит в M, найти минимальное квадратичное расстояние до элемента множества M.
4. Чтобы случайно отобрать следующий центроид $\mu^{(p)}$, используем такое распределение вероятности:

$$
P = \frac{d(\mu^{(p)}, M)^2}{\sum_i d(x^{(i)}, M)^2}
$$

Это вероятность того, что элемент $\mu^{(p)}$ будет новым центроидом.

5. Таким образом находим k центроидов.
6. Далее продолжаем как в предыдущем алгоритме.

Реализация в классе ```KMeans```,
параметр ```init``` нужно ставить в init.

## Жесткая и мягкая кластеризация

Жесткая кластеризация каждому элементу сопоставляет ровно один кластер.
А мягкая каждому элементу сопоставляет вероятности принадлежности всем кластерам,
то есть в итоге можно сопоставить несколько кластеров с большой вероятностью.

## Оценка качества модели

Внутри-кластерная сумма квадратичных ошибок - SSE.

$$
SSE = \sum_{i=1}^n \sum_{j=1}^k w^{(i, j)} |\vec x^{(i)}-\vec \mu^{(j)}|_2^2
$$

Здесь $w^{(i, j)}$ равно 1, если i-ый элемент находится в кластере j, иначе 0.

## Оценка оптимального числа кластеров
(Метод локтя)

Чтобы оценить оптимальное количество кластеров, нужно построить зависимость
SSE от числа кластеров и там все будет видно.

## Силуэтные графики

Еще один способ оценки качества кластеризации - это построить силуэтный график.

$a^{(i)}$ - среднее расстояние между данным образцом и всеми точками его кластера.
$b^{(i)}$ - среднее расстояние между данным образцом и всеми точками второго ближайшего к нему кластера.

Силуэт:

$$
s^{(i)} = \frac{b^{(i)} - a^{(i)}}{max{b^{(i)}, a^{(i)}}}
$$

В библиотеке есть метод ```silhouette_samples```.

## Кластерные деревья

### Алгоритм

Агломеративая кластеризация на основе метода полной связи =
Используем метод дальнего соседа. Начинаем с одиночных кластеров.

1. Вычислить матрицу расстояний всех образцов.
2. Представить каждую точку данных как одноэлементный кластер.
3. Объединить два кластера на основе расстояния наиболее отличающихся членов.
4. Обновить матрицу расстояний.
5. Повторять предыдущие, пока не останется один кластер.

### Реализация через scipy

Реализация:
Есть метод ```linkage``` - он возвращает таблицу, с такими столбцами:
первый - номер строки первого элемента кластера,
второй - номер строки второго элемента кластера,
третий - расстояние,
четвертый - число элементов в кластере.
Отметим, что номера отсчитываются таким образом, что первые элементы в таблице - это единичные кластеры.
После можно построить дерево методом ```dendrogram```.

### Реализация через sklearn

Есть класс ```AgglomerativeClustering```.

## Плотностная кластеризация

Здесь мы будем анализировать плотность элементов.

### Алгоритм DBSCAN

Каждому элементу присвоим один из типов:

* корневая точка - в заданной $\epsilon$-окрестности лежит минимум MinPts точек.
* граничная точка - лежит в $\epsilon$-окрестности корневой, но имеет соседей меньше, чем MinPts.
* шумовая точка - остальные точки.

Сам алгоритм такой:

1. для всех корневых точек формируем кластер, или связную группу - то есть несколько корневых точек,
расстояние между которыми меньше $\epsilon$.
2. каждую граничную точку назначаем соотвествующему кластеру.

Таким образом, имеем два гиперпараметра: $\epsilon$ и MinPts.

### Реализация




