# Регрессионый анализ

Предсказываем сначение непрерывной величины.
Самый простой пример - МНК.

## Seaborn

### Графики зависимости между признаками

С помощью библиотеки seaborn можно быстро строить графики зависимости признаков друг от друга.
Используем:
```sns.pairplot```

### Представление матрицы в виде графика

Также можно легко нарисовать матрицу. Например, матрицу корреляции.
Используем:
```sns.heatmap```

## Матрицы

### Ковариационная матрица

$$
cov(
\begin{pmatrix}
x_1 \\
x_2
\end{pmatrix},
\begin{pmatrix}
y_1 \\
y_2
\end{pmatrix}
) =
\begin{pmatrix}
\sigma_{x_1 y_1} & \sigma{x_1 y_2} \\
\sigma_{x_2 y_1} & \sigma{x_2 y_2}
\end{pmatrix}
$$

где

$$
\sigma_{x_i y_j} = \overline{(x_i - \overline{x_i})(y_j - \overline{y_j})}
$$

Показывает распределение векторов.

### Корреляционная матрица

Аналогично предыдущей, только компоненты нормированны. И по модулю меньше единицы.
Если коэф. корреляции равен единице, значит есть линейная зависимость.

$$
\sigma_{xy}' = \frac{\sigma_{xy}}{\sigma_{x} \sigma_{y}}
$$

### Вычисление коэф корреляции

В numpy есть метод ```np.corrcoef``` - он вычисляет матрицу корреляции.

## Линейная регрессия

В общем многомерном случае формула для весов такая:

$$
\vec \omega = (X^T X)^{-1} X^T y
$$

Или используем библиотечную функцию:

```
LinearRegression().fit(X, y).predict(X_new)
```

## Выбросы

Некоторые точки слишком далеко от прямой МНК - они называются выбросы,
с ними надо что-то делать. Иначе прямая будет уезжать от главной зависимости.

### Алгоритм RANSAC

1. Выбрать случайное число образцов и назвать их не-выбросами. Подогнать на них модель.
2. Проверить остальные точки, если они лежат на допустимом расстоянии от гиперплоскости, то
добавить их в не-выбросы.
3. Еще раз обучить модель на всех не-выбросах.
4. Оценить ошибку, если все ок, то закончить. Если нет, то начать сначала.

## Оценка качества модели

### Остатки

Можно просто посчитать разность предсказания с реальным ответом.
Главное, чтобы отклонения были случайными вокруг модели, иначе
есть какая-то другая зависимость, которую мы не уловили.

### MSE

```
mean_squared_error(y_train, y_test)
```

Считает среднеквадратичную ошибку.

### R2

$$
R^2 = 1 - \frac{MSE}{\sigma_y}
$$

```
r2_score(y_train, y_test)
```

## Регуляризация в линейной регрессии

Используем в случае проблемы переобучения.

Виды:

* Гребневая - L2 штраф - ```Ridge()```
* Lasso - L1 штраф - ```Lasso()```
* elastic net - оба штрафа - ```ElasticNet()```

## Полиномиальная регрессия

Теперь ищем оптимальные коэффициенты для ряда.

$$
y = w_0 + w_1 x + w_2 x^2 + ...
$$

Есть класс ```PolynomialFeatures(degree)```, который преобразовывает данные
в новые данные после полиномизирования.

Пример degree=2: было - [a, b], станет - [1, a, b, a^2, ab, b^2]

А дальше к этим данныи уже можно применять обычную линейную регрессию.

## Регрессия на основе дерева решений

Используем теорию, аналогичную классификатору из дерева решений.
Есть класс ```DecisionTreeRegression```.

## Регрессия на основе случайного леса

Случайные леса менее чувствительны к выбросам.
Есть класс ```RandomForestRegressor```